***************
** Arguments **
***************
backbone: 
calibration: True
config_file: configs/trainers/CoOp/rn50.yaml
cutrate: 0.5
dataset: oxford_flowers
dataset_config_file: configs/datasets/oxford_flowers.yaml
eval_only: True
head: 
load_epoch: None
model_dir: 
no_train: False
opts: []
output_dir: output/ZeroshotCLIP3/rn50/oxford_flowers
relevance_refine: False
resume: 
root: /ix/yufeihuang/jia/nlp/prompt/data
seed: -1
source_domains: None
target_domains: None
trainer: ZeroshotCLIP3
transforms: None
vb_dir: /ihome/yufeihuang/jiy130/Prompt/CoOp/knowledgebase
vbsize: 40
verbalizer: True
************
** Config **
************
CALIBRATION: True
CALIBRATION_CUT: 0.5
DATALOADER:
  K_TRANSFORMS: 1
  NUM_WORKERS: 8
  RETURN_IMG0: False
  TEST:
    BATCH_SIZE: 100
    SAMPLER: SequentialSampler
  TRAIN_U:
    BATCH_SIZE: 32
    N_DOMAIN: 0
    N_INS: 16
    SAME_AS_X: True
    SAMPLER: RandomSampler
  TRAIN_X:
    BATCH_SIZE: 32
    N_DOMAIN: 0
    N_INS: 16
    SAMPLER: RandomSampler
DATASET:
  ALL_AS_UNLABELED: False
  CIFAR_C_LEVEL: 1
  CIFAR_C_TYPE: 
  NAME: OxfordFlowers
  NUM_LABELED: -1
  NUM_SHOTS: -1
  ROOT: /ix/yufeihuang/jia/nlp/prompt/data
  SOURCE_DOMAINS: ()
  STL10_FOLD: -1
  SUBSAMPLE_CLASSES: all
  TARGET_DOMAINS: ()
  VAL_PERCENT: 0.1
INPUT:
  COLORJITTER_B: 0.4
  COLORJITTER_C: 0.4
  COLORJITTER_H: 0.1
  COLORJITTER_S: 0.4
  CROP_PADDING: 4
  CUTOUT_LEN: 16
  CUTOUT_N: 1
  GB_K: 21
  GB_P: 0.5
  GN_MEAN: 0.0
  GN_STD: 0.15
  INTERPOLATION: bicubic
  NO_TRANSFORM: False
  PIXEL_MEAN: [0.48145466, 0.4578275, 0.40821073]
  PIXEL_STD: [0.26862954, 0.26130258, 0.27577711]
  RANDAUGMENT_M: 10
  RANDAUGMENT_N: 2
  RGS_P: 0.2
  RRCROP_SCALE: (0.08, 1.0)
  SIZE: (224, 224)
  TRANSFORMS: ('random_resized_crop', 'random_flip', 'normalize')
MODEL:
  BACKBONE:
    NAME: RN50
    PRETRAINED: True
  HEAD:
    ACTIVATION: relu
    BN: True
    DROPOUT: 0.0
    HIDDEN_LAYERS: ()
    NAME: 
  INIT_WEIGHTS: 
OPTIM:
  ADAM_BETA1: 0.9
  ADAM_BETA2: 0.999
  BASE_LR_MULT: 0.1
  GAMMA: 0.1
  LR: 0.002
  LR_SCHEDULER: cosine
  MAX_EPOCH: 200
  MOMENTUM: 0.9
  NAME: sgd
  NEW_LAYERS: ()
  RMSPROP_ALPHA: 0.99
  SGD_DAMPNING: 0
  SGD_NESTEROV: False
  STAGED_LR: False
  STEPSIZE: (-1,)
  WARMUP_CONS_LR: 1e-05
  WARMUP_EPOCH: 1
  WARMUP_MIN_LR: 1e-05
  WARMUP_RECOUNT: True
  WARMUP_TYPE: constant
  WEIGHT_DECAY: 0.0005
OUTPUT_DIR: output/ZeroshotCLIP3/rn50/oxford_flowers
RELEVANCE_REFINE: False
RESUME: 
SEED: -1
TEST:
  COMPUTE_CMAT: False
  EVALUATOR: Classification
  FINAL_MODEL: last_step
  NO_TEST: False
  PER_CLASS_RESULT: False
  SPLIT: test
TRAIN:
  CHECKPOINT_FREQ: 0
  COUNT_ITER: train_x
  PRINT_FREQ: 5
TRAINER:
  CDAC:
    CLASS_LR_MULTI: 10
    P_THRESH: 0.95
    RAMPUP_COEF: 30
    RAMPUP_ITRS: 1000
    STRONG_TRANSFORMS: ()
    TOPK_MATCH: 5
  COCOOP:
    CTX_INIT: 
    N_CTX: 16
    PREC: fp16
  COOP:
    CLASS_TOKEN_POSITION: end
    CSC: False
    CTX_INIT: 
    N_CTX: 16
    PREC: fp16
  CROSSGRAD:
    ALPHA_D: 0.5
    ALPHA_F: 0.5
    EPS_D: 1.0
    EPS_F: 1.0
  DAEL:
    CONF_THRE: 0.95
    STRONG_TRANSFORMS: ()
    WEIGHT_U: 0.5
  DAELDG:
    CONF_THRE: 0.95
    STRONG_TRANSFORMS: ()
    WEIGHT_U: 0.5
  DDAIG:
    ALPHA: 0.5
    CLAMP: False
    CLAMP_MAX: 1.0
    CLAMP_MIN: -1.0
    G_ARCH: 
    LMDA: 0.3
    WARMUP: 0
  DOMAINMIX:
    ALPHA: 1.0
    BETA: 1.0
    TYPE: crossdomain
  ENTMIN:
    LMDA: 0.001
  FIXMATCH:
    CONF_THRE: 0.95
    STRONG_TRANSFORMS: ()
    WEIGHT_U: 1.0
  M3SDA:
    LMDA: 0.5
    N_STEP_F: 4
  MCD:
    N_STEP_F: 4
  MEANTEACHER:
    EMA_ALPHA: 0.999
    RAMPUP: 5
    WEIGHT_U: 1.0
  MIXMATCH:
    MIXUP_BETA: 0.75
    RAMPUP: 20000
    TEMP: 2.0
    WEIGHT_U: 100.0
  MME:
    LMDA: 0.1
  NAME: ZeroshotCLIP3
  SE:
    CONF_THRE: 0.95
    EMA_ALPHA: 0.999
    RAMPUP: 300
USE_CUDA: True
VERBALIZER:
  DIR: /ihome/yufeihuang/jiy130/Prompt/CoOp/knowledgebase/oxford_flowers
  SIZE: 40
VERBOSE: True
VERSION: 1
Collecting env info ...
** System info **
PyTorch version: 1.13.1
Is debug build: False
CUDA used to build PyTorch: 11.7
ROCM used to build PyTorch: N/A

OS: Red Hat Enterprise Linux (x86_64)
GCC version: (GCC) 4.8.5 20150623 (Red Hat 4.8.5-44)
Clang version: Could not collect
CMake version: Could not collect
Libc version: glibc-2.17

Python version: 3.8.16 (default, Jan 17 2023, 23:13:24)  [GCC 11.2.0] (64-bit runtime)
Python platform: Linux-3.10.0-1160.71.1.el7.x86_64-x86_64-with-glibc2.17
Is CUDA available: True
CUDA runtime version: Could not collect
CUDA_MODULE_LOADING set to: LAZY
GPU models and configuration: 
GPU 0: NVIDIA A100-PCIE-40GB
GPU 1: NVIDIA A100-PCIE-40GB
GPU 2: NVIDIA A100-PCIE-40GB
GPU 3: NVIDIA A100-PCIE-40GB

Nvidia driver version: 515.65.01
cuDNN version: Could not collect
HIP runtime version: N/A
MIOpen runtime version: N/A
Is XNNPACK available: True

Versions of relevant libraries:
[pip3] numpy==1.23.5
[pip3] torch==1.13.1
[pip3] torchaudio==0.13.1
[pip3] torchvision==0.14.1
[conda] blas                      1.0                         mkl  
[conda] cudatoolkit               11.3.1               h2bc3f7f_2  
[conda] ffmpeg                    4.3                  hf484d3e_0    pytorch
[conda] mkl                       2021.4.0           h06a4308_640  
[conda] mkl-service               2.4.0            py38h7f8727e_0  
[conda] mkl_fft                   1.3.1            py38hd3c417c_0  
[conda] mkl_random                1.2.2            py38h51133e4_0  
[conda] numpy                     1.23.5           py38h14f4228_0  
[conda] numpy-base                1.23.5           py38h31eccc5_0  
[conda] pytorch                   1.13.1          py3.8_cuda11.7_cudnn8.5.0_0    pytorch
[conda] pytorch-cuda              11.7                 h778d358_3    pytorch
[conda] pytorch-mutex             1.0                        cuda    pytorch
[conda] torchaudio                0.13.1               py38_cu117    pytorch
[conda] torchvision               0.14.1               py38_cu117    pytorch
        Pillow (9.4.0)

Loading trainer: ZeroshotCLIP3
Loading dataset: OxfordFlowers
Reading split from /ix/yufeihuang/jia/nlp/prompt/data/oxford_flowers/split_zhou_OxfordFlowers.json
Building transform_train
+ random resized crop (size=(224, 224), scale=(0.08, 1.0))
+ random flip
+ to torch tensor of range [0, 1]
+ normalization (mean=[0.48145466, 0.4578275, 0.40821073], std=[0.26862954, 0.26130258, 0.27577711])
Building transform_test
+ resize the smaller edge to 224
+ 224x224 center crop
+ to torch tensor of range [0, 1]
+ normalization (mean=[0.48145466, 0.4578275, 0.40821073], std=[0.26862954, 0.26130258, 0.27577711])
---------  -------------
Dataset    OxfordFlowers
# classes  102
# train_x  4,093
# val      1,633
# test     2,463
---------  -------------
Loading CLIP (backbone: RN50)
Selected examples (mixed) [2440, 2987, 1578, 1672, 899, 2404, 674, 1387, 2193, 293, 2378, 2113, 3190, 703, 1972, 3453, 845, 843, 1530, 2180, 3043, 2782, 623, 2227, 1577, 3165, 3733, 1881, 18, 2785, 3802, 496, 2976, 2500, 1963, 3673, 967, 1765, 1538, 49, 925, 3948, 562, 3535, 692, 2494, 1912, 2961, 1822, 1112, 4000, 315, 3162, 915, 133, 584, 583, 2937, 534, 3425, 1604, 371, 2633, 906, 2514, 944, 1062, 188, 2216, 3124, 3934, 829, 2304, 3157, 3721, 1152, 87, 1473, 1824, 3028, 1627, 1842, 1968, 79, 822, 257, 742, 3048, 910, 3186, 224, 448, 2766, 1999, 3313, 3650, 3007, 27, 50, 3642, 2625, 3328, 3388, 945, 3475, 35, 281, 2507, 1537, 1504, 3713, 2996, 1462, 931, 1308, 2532, 777, 1044, 2765, 1367, 124, 2617, 1050, 3946, 2670, 3853, 85, 2671, 435, 1214, 1018, 2200, 2778, 1705, 2363, 959, 1272, 1539, 992, 1038, 1212, 3777, 2672, 3272, 3626, 1628, 2975, 203, 2510, 53, 1015, 2727, 1119, 3605, 1731, 204, 3554, 593, 4004, 3231, 749, 828, 3947, 3898, 4014, 730, 2849, 3132, 2806, 1509, 3092, 1556, 1412, 2271, 111, 2460, 701, 2097, 2313, 2477, 535, 4080, 3831, 3783, 1312, 2926, 231, 3305, 210, 3971, 2592, 1658, 1818, 2099, 856, 2639, 2522, 2127, 1455, 832]
Building transform_test
+ resize the smaller edge to 224
+ 224x224 center crop
+ to torch tensor of range [0, 1]
+ normalization (mean=[0.48145466, 0.4578275, 0.40821073], std=[0.26862954, 0.26130258, 0.27577711])
Load support dataloader successfully! size: 200
num of org label words: 1609
cali text features shape: torch.Size([1609, 1024])
the calibration logits is tensor([[18.2656, 23.0781, 24.7812,  ..., 25.4375, 23.2812, 23.0938],
        [21.0312, 25.3906, 25.2500,  ..., 24.4062, 24.9062, 24.0156],
        [16.5156, 21.0938, 24.5000,  ..., 25.8281, 24.0625, 21.6875],
        ...,
        [22.9688, 25.3281, 26.0938,  ..., 28.0625, 24.5156, 23.7344],
        [20.8281, 18.5625, 23.9688,  ..., 24.0312, 24.5156, 22.1875],
        [20.7188, 17.5312, 18.5156,  ..., 18.2500, 22.0625, 19.4062]],
       device='cuda:0', dtype=torch.float16, grad_fn=<MmBackward0>)
cc_logits shape: torch.Size([200, 1609])
Phase 1 [20, 3, 8, 7, 22, 7, 27, 41, 14, 35, 3, 1, 38, 22, 20, 12, 19, 31, 20, 18, 1, 6, 32, 19, 19, 28, 1, 1, 19, 24, 17, 5, 1, 13, 13, 1, 1, 1, 8, 19, 1, 18, 20, 10, 5, 8, 1, 9, 15, 22, 13, 28, 21, 15, 16, 41, 3, 4, 1, 1, 9, 19, 39, 39, 23, 8, 6, 23, 6, 21, 4, 2, 28, 41, 33, 38, 22, 11, 2, 8, 23, 22, 14, 6, 1, 26, 13, 24, 22, 6, 17, 28, 20, 33, 10, 18, 25, 8, 7, 10, 36, 9]

register_calibrate_logits starting shape: torch.Size([1609])
logits shape -1 :1609
rm_calibrate_ids: {0, 1, 6, 8, 9, 10, 14, 20, 23, 31, 32, 33, 35, 38, 40, 44, 47, 49, 50, 54, 55, 60, 62, 63, 64, 65, 66, 67, 86, 94, 96, 97, 100, 101, 102, 104, 105, 106, 107, 108, 109, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 126, 127, 129, 133, 135, 136, 139, 140, 141, 142, 143, 146, 149, 150, 163, 168, 175, 184, 185, 186, 187, 188, 190, 191, 203, 210, 226, 227, 228, 229, 230, 232, 233, 234, 235, 236, 238, 239, 240, 241, 242, 243, 244, 245, 246, 247, 248, 250, 251, 252, 255, 256, 259, 260, 261, 264, 267, 269, 270, 271, 272, 273, 274, 275, 277, 278, 280, 293, 294, 295, 296, 299, 301, 302, 303, 310, 316, 321, 322, 330, 332, 333, 334, 336, 337, 338, 343, 345, 346, 347, 350, 361, 362, 366, 367, 368, 369, 370, 371, 373, 374, 375, 376, 377, 378, 379, 380, 381, 382, 383, 385, 386, 387, 388, 392, 393, 394, 396, 397, 398, 399, 400, 402, 404, 407, 408, 410, 411, 413, 414, 415, 417, 418, 420, 421, 422, 424, 425, 426, 429, 430, 432, 434, 437, 439, 440, 441, 442, 445, 446, 447, 448, 449, 450, 452, 453, 454, 455, 456, 458, 460, 461, 462, 463, 464, 467, 468, 469, 470, 472, 473, 474, 475, 476, 477, 478, 479, 480, 481, 482, 483, 484, 485, 486, 487, 488, 489, 492, 494, 501, 505, 510, 511, 512, 517, 518, 520, 521, 523, 527, 533, 535, 537, 539, 540, 541, 543, 545, 546, 548, 549, 551, 552, 554, 564, 567, 569, 570, 578, 579, 582, 586, 587, 589, 590, 597, 598, 599, 600, 602, 603, 605, 606, 607, 610, 611, 612, 613, 616, 617, 618, 619, 622, 624, 625, 627, 629, 630, 631, 633, 635, 636, 637, 639, 641, 643, 644, 645, 647, 649, 651, 652, 654, 656, 659, 660, 661, 662, 666, 668, 669, 670, 671, 673, 675, 676, 677, 678, 679, 680, 681, 683, 684, 685, 686, 688, 689, 690, 691, 692, 693, 694, 696, 706, 708, 709, 710, 711, 712, 713, 714, 715, 716, 717, 718, 719, 721, 724, 725, 726, 732, 733, 739, 742, 743, 744, 745, 746, 747, 749, 750, 751, 752, 753, 754, 755, 756, 757, 758, 768, 769, 770, 771, 772, 773, 774, 775, 776, 777, 778, 779, 780, 781, 782, 783, 785, 786, 787, 788, 789, 793, 794, 795, 797, 801, 807, 810, 833, 839, 840, 842, 843, 844, 845, 847, 848, 849, 853, 858, 861, 864, 866, 873, 877, 880, 882, 883, 884, 885, 900, 901, 903, 908, 909, 911, 913, 914, 916, 917, 919, 920, 921, 924, 931, 942, 952, 955, 964, 978, 979, 982, 984, 985, 986, 989, 990, 992, 994, 995, 997, 998, 999, 1003, 1005, 1006, 1007, 1008, 1011, 1012, 1014, 1015, 1016, 1017, 1018, 1021, 1023, 1028, 1029, 1031, 1032, 1034, 1037, 1038, 1042, 1043, 1045, 1046, 1047, 1048, 1051, 1052, 1054, 1058, 1059, 1060, 1062, 1064, 1065, 1067, 1069, 1071, 1072, 1076, 1112, 1114, 1117, 1118, 1119, 1120, 1121, 1122, 1123, 1124, 1125, 1126, 1127, 1129, 1130, 1132, 1135, 1136, 1137, 1138, 1139, 1140, 1143, 1144, 1145, 1146, 1147, 1149, 1150, 1151, 1156, 1157, 1161, 1163, 1164, 1165, 1167, 1168, 1169, 1178, 1181, 1184, 1185, 1186, 1188, 1189, 1190, 1193, 1194, 1195, 1198, 1199, 1203, 1210, 1212, 1213, 1214, 1215, 1216, 1217, 1218, 1219, 1221, 1223, 1226, 1227, 1228, 1229, 1231, 1232, 1233, 1235, 1236, 1239, 1240, 1244, 1248, 1249, 1253, 1254, 1255, 1257, 1258, 1259, 1260, 1261, 1262, 1263, 1264, 1265, 1269, 1270, 1273, 1274, 1275, 1276, 1277, 1278, 1282, 1283, 1285, 1286, 1288, 1289, 1290, 1291, 1294, 1295, 1296, 1297, 1298, 1299, 1300, 1301, 1302, 1303, 1304, 1305, 1306, 1307, 1308, 1309, 1310, 1312, 1314, 1315, 1316, 1320, 1321, 1322, 1323, 1324, 1325, 1326, 1327, 1328, 1332, 1333, 1334, 1335, 1336, 1337, 1339, 1340, 1341, 1342, 1343, 1344, 1345, 1346, 1350, 1351, 1352, 1353, 1354, 1355, 1356, 1357, 1358, 1359, 1360, 1361, 1362, 1363, 1365, 1366, 1367, 1368, 1370, 1371, 1372, 1373, 1374, 1375, 1376, 1377, 1378, 1379, 1380, 1381, 1382, 1383, 1386, 1388, 1389, 1390, 1395, 1402, 1405, 1406, 1407, 1408, 1409, 1410, 1411, 1414, 1416, 1420, 1423, 1433, 1435, 1436, 1437, 1440, 1441, 1442, 1443, 1444, 1446, 1447, 1452, 1453, 1454, 1455, 1456, 1459, 1461, 1462, 1463, 1465, 1466, 1468, 1476, 1479, 1486, 1487, 1488, 1490, 1492, 1493, 1494, 1495, 1496, 1498, 1500, 1501, 1504, 1505, 1506, 1507, 1508, 1510, 1512, 1513, 1514, 1515, 1516, 1517, 1518, 1519, 1520, 1521, 1522, 1524, 1525, 1526, 1527, 1528, 1529, 1530, 1531, 1532, 1533, 1538, 1539, 1543, 1544, 1546, 1547, 1549, 1554, 1555, 1556, 1557, 1564, 1565, 1566, 1567, 1569, 1570, 1571, 1572, 1573, 1577, 1578, 1579, 1581, 1584, 1592, 1593, 1596, 1599, 1600, 1601, 1605, 1608}, shape 804
